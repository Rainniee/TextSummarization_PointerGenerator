{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot TF Seq2Seq Modeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainniee/TextSummarization_PointerGenerator/blob/master/Chatbot%20Modeling_Seq2SeqTF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Io2gKLDQ1r1I",
        "colab_type": "code",
        "outputId": "acada17b-2fff-41e4-8237-1bde395a64eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oRcUFky71y6i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"drive/My Drive/Assignment 4 Chatbot/articles_abstract_pairs/article-abstract-tokenized.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mWRjRBq14dk",
        "colab_type": "code",
        "outputId": "bbde3c19-62f8-47e6-aff4-1ac206ff72a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Dense, Input, RepeatVector, concatenate, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.ops.rnn import dynamic_rnn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gx9SHK8c17w9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RNN_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "KEEP_PROBABILITY = 0.5\n",
        "OPTIMIZER_TYPE = 'adam'\n",
        "LEARNING_RATE = 0.001\n",
        "EMBEDDING_SIZE = 100\n",
        "\n",
        "data_dir_path = 'drive/My Drive/Assignment 4 Chatbot'\n",
        "data_file = '/articles_abstract_pairs/article-abstract-tokenized.csv'\n",
        "glove_file = '/articles_abstract_pairs/glove.6B.' + str(EMBEDDING_SIZE) + 'd.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztUmrsDW17zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VERBOSE = 1\n",
        "NUM_LAYERS = 3\n",
        "MAX_INPUT_SEQ_LENGTH = 500\n",
        "MAX_TARGET_SEQ_LENGTH = 50\n",
        "MAX_INPUT_VOCAB_SIZE = 3000\n",
        "MAX_TARGET_VOCAB_SIZE = 1000\n",
        "NUM_SAMPLES = 5000\n",
        "MAX_DECODER_SEQ_LENGTH = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zoVL-3jJ1727",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_keras_optimizer():\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, momentum=0.0, decay=0.0, nesterov=False)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.Adagrad(lr=LEARNING_RATE, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        keras_optimizer = keras.optimizers.Adadelta(lr=LEARNING_RATE, rho=0.95, epsilon=None, decay=0.0)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                                                amsgrad=False)\n",
        "\n",
        "    return keras_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eKQqc8W2Eh7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_tf_optimizer(lr):\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.RMSPropOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.AdagradOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        tf_optimizer = tf.train.AdadeltaOptimizer(lr)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    return tf_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CbhtmhE52RUa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Modeling"
      ]
    },
    {
      "metadata": {
        "id": "YsxMNBkb2SsH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_data(input_texts, target_texts):\n",
        "    vocab = set()\n",
        "    vocab.add('<PAD>')\n",
        "    vocab.add('END')\n",
        "    vocab.add('<UNK>')\n",
        "    vocab.add('START')\n",
        "\n",
        "    for input_text in input_texts:\n",
        "        for word in input_text:\n",
        "            if word not in vocab:\n",
        "                vocab.add(word)\n",
        "    for target_text in target_texts:\n",
        "        for word in target_text:\n",
        "            if word not in vocab:\n",
        "                vocab.add(word)\n",
        "\n",
        "    vocab = sorted(list(vocab))\n",
        "    word2idx = dict([(word, i) for i, word in enumerate(vocab)])\n",
        "    idx2word = dict((i, word) for word, i in word2idx.items())\n",
        "\n",
        "    source_text_id = []\n",
        "    target_text_id = []\n",
        "\n",
        "    for i in range(len(input_texts)):\n",
        "        source_sentence = input_texts[i]\n",
        "        target_sentence = target_texts[i]\n",
        "\n",
        "        source_token_id = []\n",
        "        target_token_id = []\n",
        "\n",
        "        for index, token in enumerate(source_sentence):\n",
        "            if token != \"\":\n",
        "                source_token_id.append(word2idx[token])\n",
        "\n",
        "        for index, token in enumerate(target_sentence):\n",
        "            if token != \"\":\n",
        "                target_token_id.append(word2idx[token])\n",
        "\n",
        "        target_token_id.append(word2idx['END'])\n",
        "\n",
        "        source_text_id.append(source_token_id)\n",
        "        target_text_id.append(target_token_id)\n",
        "\n",
        "    config = dict()\n",
        "    config['word2idx'] = word2idx\n",
        "    config['idx2word'] = idx2word\n",
        "    config['source_text_id'] = source_text_id\n",
        "    config['target_text_id'] = target_text_id\n",
        "\n",
        "    return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChDNrPma4a4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(target, logits):\n",
        "    max_seq = max(target.shape[1], logits.shape[1])\n",
        "    if max_seq - target.shape[1]:\n",
        "        target = np.pad(\n",
        "            target,\n",
        "            [(0, 0), (0, max_seq - target.shape[1])],\n",
        "            'constant')\n",
        "    if max_seq - logits.shape[1]:\n",
        "        logits = np.pad(\n",
        "            logits,\n",
        "            [(0, 0), (0, max_seq - logits.shape[1])],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(target, logits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "439Iqabj2V_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TfSeq2Seq(object):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.word2idx = config['word2idx']\n",
        "        self.idx2word = config['idx2word']\n",
        "        self.source_text_id = config['source_text_id']\n",
        "        self.target_text_id = config['target_text_id']\n",
        "\n",
        "    def encoding_layer(self, input_data, keep_prob):\n",
        "        embed = tf.contrib.layers.embed_sequence(input_data,\n",
        "                                                 vocab_size=len(self.word2idx),\n",
        "                                                 embed_dim=EMBEDDING_SIZE)\n",
        "\n",
        "        stacked_cells = tf.contrib.rnn.MultiRNNCell(\n",
        "            [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(RNN_SIZE), keep_prob) for _ in\n",
        "             range(NUM_LAYERS)])\n",
        "\n",
        "        outputs, state = dynamic_rnn(stacked_cells, embed, dtype=tf.float32)\n",
        "        return outputs, state\n",
        "\n",
        "    def process_decoder_input(self, targets):\n",
        "        go_id = self.word2idx['START']\n",
        "\n",
        "        after_slice = tf.strided_slice(targets, [0, 0], [BATCH_SIZE, -1], [1, 1])\n",
        "        after_concat = tf.concat([tf.fill([BATCH_SIZE, 1], go_id), after_slice], 1)\n",
        "\n",
        "        return after_concat\n",
        "\n",
        "    def decoding_layer(self, dec_input, encoder_state, keep_prob, target_sequence_length, max_target_len):\n",
        "        target_vocab_size = len(self.word2idx)\n",
        "        dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, EMBEDDING_SIZE]))\n",
        "        dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "\n",
        "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(RNN_SIZE) for _ in range(NUM_LAYERS)])\n",
        "\n",
        "        with tf.variable_scope(\"decode\"):\n",
        "            output_layer = tf.layers.Dense(target_vocab_size)\n",
        "            dec_cell_train = tf.contrib.rnn.DropoutWrapper(cells, output_keep_prob=keep_prob)\n",
        "\n",
        "            helper_train = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_sequence_length)\n",
        "\n",
        "            decoder_train = tf.contrib.seq2seq.BasicDecoder(dec_cell_train, helper_train, encoder_state,\n",
        "                                                            output_layer)\n",
        "\n",
        "            train_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_train, impute_finished=True,\n",
        "                                                                   maximum_iterations=max_target_len)\n",
        "\n",
        "        with tf.variable_scope(\"decode\", reuse=True):\n",
        "            start_sequence_id = self.word2idx['START']\n",
        "            end_sequence_id = self.word2idx['END']\n",
        "\n",
        "            dec_cell_infer = tf.contrib.rnn.DropoutWrapper(cells, output_keep_prob=keep_prob)\n",
        "\n",
        "            helper_infer = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, tf.fill([BATCH_SIZE],\n",
        "                                                                                            start_sequence_id),\n",
        "                                                                    end_sequence_id)\n",
        "\n",
        "            decoder_infer = tf.contrib.seq2seq.BasicDecoder(dec_cell_infer, helper_infer, encoder_state, output_layer)\n",
        "\n",
        "            infer_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_infer, impute_finished=True,\n",
        "                                                                   maximum_iterations=max_target_len)\n",
        "\n",
        "        return train_output, infer_output\n",
        "\n",
        "    def seq2seq_model(self, input_data, targets, keep_prob, target_sequence_length, max_target_len):\n",
        "\n",
        "        enc_outputs, enc_states = self.encoding_layer(input_data, keep_prob)\n",
        "\n",
        "        dec_input = self.process_decoder_input(targets)\n",
        "\n",
        "        train_output, infer_output = self.decoding_layer(dec_input, enc_states, keep_prob, target_sequence_length,\n",
        "                                                         max_target_len)\n",
        "\n",
        "        return train_output, infer_output\n",
        "\n",
        "    def get_batches(self, sources, targets):\n",
        "        for batch_i in range(0, len(sources) // BATCH_SIZE):\n",
        "            start_i = batch_i * BATCH_SIZE\n",
        "\n",
        "            sources_batch = sources[start_i:start_i + BATCH_SIZE]\n",
        "            targets_batch = targets[start_i:start_i + BATCH_SIZE]\n",
        "\n",
        "            pad_int = self.word2idx['<PAD>']\n",
        "            max_sentence_source = max([len(sentence) for sentence in sources_batch])\n",
        "            max_sentence_target = max([len(sentence) for sentence in targets_batch])\n",
        "            pad_sources_batch = np.array([sentence + [pad_int] * (max_sentence_source - len(sentence)) for sentence in\n",
        "                                          sources_batch])\n",
        "            pad_targets_batch = np.array([sentence + [pad_int] * (max_sentence_target - len(sentence)) for sentence in\n",
        "                                          targets_batch])\n",
        "\n",
        "            pad_targets_lengths = []\n",
        "            for target in pad_targets_batch:\n",
        "                pad_targets_lengths.append(len(target))\n",
        "\n",
        "            pad_source_lengths = []\n",
        "            for source in pad_sources_batch:\n",
        "                pad_source_lengths.append(len(source))\n",
        "\n",
        "            yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n",
        "\n",
        "    def fit(self):\n",
        "        train_graph = tf.Graph()\n",
        "        with train_graph.as_default():\n",
        "            inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "            input_data = tf.reverse(inputs, [-1])\n",
        "            targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "            target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
        "            max_target_len = tf.reduce_max(target_sequence_length)\n",
        "            lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
        "            keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "            train_logits, inference_logits = self.seq2seq_model(input_data, targets, keep_prob, target_sequence_length,\n",
        "                                                                max_target_len)\n",
        "            training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
        "            inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "            masks = tf.sequence_mask(target_sequence_length, max_target_len, dtype=tf.float32, name='masks')\n",
        "\n",
        "            with tf.name_scope(\"optimization\"):\n",
        "\n",
        "                cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
        "                # optimizer = tf.train.AdamOptimizer(lr_rate)\n",
        "                optimizer = def_tf_optimizer(lr_rate)\n",
        "                gradients = optimizer.compute_gradients(cost)\n",
        "                capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not\n",
        "                                    None]\n",
        "                train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "            train_source = self.source_text_id[BATCH_SIZE:]\n",
        "            train_target = self.target_text_id[BATCH_SIZE:]\n",
        "            valid_source = self.source_text_id[:BATCH_SIZE]\n",
        "            valid_target = self.target_text_id[:BATCH_SIZE]\n",
        "\n",
        "            (valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths) = \\\n",
        "                next(self.get_batches(valid_source, valid_target))\n",
        "\n",
        "            sess = tf.Session(graph=train_graph)\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            for epoch_i in range(EPOCHS):\n",
        "                for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
        "                        self.get_batches(train_source, train_target)):\n",
        "                    _, loss = sess.run([train_op, cost], {inputs: source_batch, targets: target_batch,\n",
        "                                                          lr_rate: LEARNING_RATE,\n",
        "                                                          target_sequence_length: targets_lengths,\n",
        "                                                          keep_prob: KEEP_PROBABILITY})\n",
        "\n",
        "                    if batch_i > 0:\n",
        "                        if batch_i % 5 == 0:\n",
        "                            batch_train_logits = sess.run(\n",
        "                             inference_logits, {input_data: source_batch, target_sequence_length: targets_lengths,\n",
        "                                                keep_prob: 1.0})\n",
        "\n",
        "                            batch_valid_logits = sess.run(\n",
        "                                inference_logits, {inputs: valid_sources_batch,\n",
        "                                                   target_sequence_length:  valid_targets_lengths,\n",
        "                                                   keep_prob: 1.0})\n",
        "\n",
        "                            train_acc = get_accuracy(target_batch, batch_train_logits)\n",
        "                            valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
        "\n",
        "                            print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}'\n",
        "                                  ', Loss: {:>6.4f}'.format(epoch_i, batch_i, len(self.source_text_id) // BATCH_SIZE,\n",
        "                                                            train_acc, valid_acc, loss))\n",
        "        return target_sequence_length, keep_prob, inputs, sess, inference_logits\n",
        "\n",
        "    def summarize(self, inputs, text, sess, inference_logits, target_sequence_length, keep_prob):\n",
        "        translate_sentence = []\n",
        "        for word in text:\n",
        "            if word in self.word2idx:\n",
        "                translate_sentence.append(self.word2idx[word])\n",
        "            else:\n",
        "                translate_sentence.append(self.word2idx['<UNK>'])\n",
        "\n",
        "        translate_logits = sess.run(inference_logits, {inputs: [translate_sentence] * BATCH_SIZE,\n",
        "                                                       target_sequence_length: [len(\n",
        "                                                           translate_sentence) * 2] * BATCH_SIZE,\n",
        "                                                       keep_prob: 1.0})[0]\n",
        "\n",
        "        return \" \".join([self.idx2word[i] for i in translate_logits])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "thWbscyT2swO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main_tf_seq2seq():\n",
        "    train = pd.read_csv(data_dir_path + data_file)\n",
        "    resumes = []\n",
        "    articles = []\n",
        "    for resume in train[train.columns[0]].values:\n",
        "        resumes.append(resume.split(' '))\n",
        "    for article in train[train.columns[1]].values:\n",
        "        articles.append(article.split(' '))\n",
        "\n",
        "    config = preprocess_data(articles[:NUM_SAMPLES], resumes[:NUM_SAMPLES])\n",
        "    summarizer = TfSeq2Seq(config)\n",
        "    target_sequence_length, keep_prob, inputs, sess, inference_logits = summarizer.fit()\n",
        "    for i in np.random.permutation(np.arange(len(articles)))[0:20]:\n",
        "        x = articles[i]\n",
        "        abstract = summarizer.summarize(inputs, x, sess, inference_logits, target_sequence_length, keep_prob)\n",
        "        print('Article: ', articles)\n",
        "        print('Generated Abstract: ', abstract)\n",
        "        print('Original Article: ', x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrL2AL5J3qSx",
        "colab_type": "code",
        "outputId": "8625b52b-1a21-449c-cc26-e5f6a7bc703e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        }
      },
      "cell_type": "code",
      "source": [
        "main_tf_seq2seq()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch    5/78 - Train Accuracy: 0.2982, Validation Accuracy: 0.2982, Loss: 9.9277\n",
            "Epoch   0 Batch   10/78 - Train Accuracy: 0.2244, Validation Accuracy: 0.2982, Loss: 9.4792\n",
            "Epoch   0 Batch   15/78 - Train Accuracy: 0.3112, Validation Accuracy: 0.2982, Loss: 8.5717\n",
            "Epoch   0 Batch   20/78 - Train Accuracy: 0.3546, Validation Accuracy: 0.2982, Loss: 7.6394\n",
            "Epoch   0 Batch   25/78 - Train Accuracy: 0.3817, Validation Accuracy: 0.2982, Loss: 7.0327\n",
            "Epoch   0 Batch   30/78 - Train Accuracy: 0.4336, Validation Accuracy: 0.2982, Loss: 6.2012\n",
            "Epoch   0 Batch   35/78 - Train Accuracy: 0.2060, Validation Accuracy: 0.2982, Loss: 7.2832\n",
            "Epoch   0 Batch   40/78 - Train Accuracy: 0.3086, Validation Accuracy: 0.2982, Loss: 6.4660\n",
            "Epoch   0 Batch   45/78 - Train Accuracy: 0.2330, Validation Accuracy: 0.2982, Loss: 7.0080\n",
            "Epoch   0 Batch   50/78 - Train Accuracy: 0.2372, Validation Accuracy: 0.2982, Loss: 7.0505\n",
            "Epoch   0 Batch   55/78 - Train Accuracy: 0.2656, Validation Accuracy: 0.2982, Loss: 6.7002\n",
            "Epoch   0 Batch   60/78 - Train Accuracy: 0.3307, Validation Accuracy: 0.2982, Loss: 6.1587\n",
            "Epoch   0 Batch   65/78 - Train Accuracy: 0.3786, Validation Accuracy: 0.2982, Loss: 5.8021\n",
            "Epoch   0 Batch   70/78 - Train Accuracy: 0.3333, Validation Accuracy: 0.2982, Loss: 6.1317\n",
            "Epoch   0 Batch   75/78 - Train Accuracy: 0.3490, Validation Accuracy: 0.2982, Loss: 6.2505\n",
            "Epoch   1 Batch    5/78 - Train Accuracy: 0.2982, Validation Accuracy: 0.2982, Loss: 6.1265\n",
            "Epoch   1 Batch   10/78 - Train Accuracy: 0.2244, Validation Accuracy: 0.2982, Loss: 6.6963\n",
            "Epoch   1 Batch   15/78 - Train Accuracy: 0.3112, Validation Accuracy: 0.2982, Loss: 5.8485\n",
            "Epoch   1 Batch   20/78 - Train Accuracy: 0.3546, Validation Accuracy: 0.2982, Loss: 5.4873\n",
            "Epoch   1 Batch   25/78 - Train Accuracy: 0.3817, Validation Accuracy: 0.2982, Loss: 5.5038\n",
            "Epoch   1 Batch   30/78 - Train Accuracy: 0.4336, Validation Accuracy: 0.2982, Loss: 5.0599\n",
            "Epoch   1 Batch   35/78 - Train Accuracy: 0.2060, Validation Accuracy: 0.2982, Loss: 6.8247\n",
            "Epoch   1 Batch   40/78 - Train Accuracy: 0.3086, Validation Accuracy: 0.2982, Loss: 6.0249\n",
            "Epoch   1 Batch   45/78 - Train Accuracy: 0.2330, Validation Accuracy: 0.2982, Loss: 6.5637\n",
            "Epoch   1 Batch   50/78 - Train Accuracy: 0.2372, Validation Accuracy: 0.2982, Loss: 6.5484\n",
            "Epoch   1 Batch   55/78 - Train Accuracy: 0.2656, Validation Accuracy: 0.2982, Loss: 6.2351\n",
            "Epoch   1 Batch   60/78 - Train Accuracy: 0.3307, Validation Accuracy: 0.2982, Loss: 5.7018\n",
            "Epoch   1 Batch   65/78 - Train Accuracy: 0.3786, Validation Accuracy: 0.2982, Loss: 5.3189\n",
            "Epoch   1 Batch   70/78 - Train Accuracy: 0.3333, Validation Accuracy: 0.2982, Loss: 5.6743\n",
            "Epoch   1 Batch   75/78 - Train Accuracy: 0.3490, Validation Accuracy: 0.2982, Loss: 5.8297\n",
            "Epoch   2 Batch    5/78 - Train Accuracy: 0.2982, Validation Accuracy: 0.2982, Loss: 5.8459\n",
            "Epoch   2 Batch   10/78 - Train Accuracy: 0.2244, Validation Accuracy: 0.2982, Loss: 6.4357\n",
            "Epoch   2 Batch   15/78 - Train Accuracy: 0.3112, Validation Accuracy: 0.2982, Loss: 5.6532\n",
            "Epoch   2 Batch   20/78 - Train Accuracy: 0.3546, Validation Accuracy: 0.2982, Loss: 5.3434\n",
            "Epoch   2 Batch   25/78 - Train Accuracy: 0.3817, Validation Accuracy: 0.2982, Loss: 5.2681\n",
            "Epoch   2 Batch   30/78 - Train Accuracy: 0.4336, Validation Accuracy: 0.2982, Loss: 4.7962\n",
            "Epoch   2 Batch   35/78 - Train Accuracy: 0.2060, Validation Accuracy: 0.2982, Loss: 6.5403\n",
            "Epoch   2 Batch   40/78 - Train Accuracy: 0.0000, Validation Accuracy: 0.0000, Loss: 5.8194\n",
            "Epoch   2 Batch   45/78 - Train Accuracy: 0.0000, Validation Accuracy: 0.0000, Loss: 6.3657\n",
            "Epoch   2 Batch   50/78 - Train Accuracy: 0.0000, Validation Accuracy: 0.0000, Loss: 6.3485\n",
            "Epoch   2 Batch   55/78 - Train Accuracy: 0.0000, Validation Accuracy: 0.0000, Loss: 6.0316\n",
            "Epoch   2 Batch   60/78 - Train Accuracy: 0.0000, Validation Accuracy: 0.0000, Loss: 5.5188\n",
            "Epoch   2 Batch   65/78 - Train Accuracy: 0.0072, Validation Accuracy: 0.0078, Loss: 5.1046\n",
            "Epoch   2 Batch   70/78 - Train Accuracy: 0.0065, Validation Accuracy: 0.0078, Loss: 5.5341\n",
            "Epoch   2 Batch   75/78 - Train Accuracy: 0.0078, Validation Accuracy: 0.0091, Loss: 5.6014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}