{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot Seq2Seq Modeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainniee/TextSummarization_PointerGenerator/blob/master/Chatbot%20Modeling_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "g0361kiLeSFT",
        "colab_type": "code",
        "outputId": "3bfa9585-b55f-49df-b3f5-be0b8f963784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NVHNIYOFfAu1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"drive/My Drive/Assignment 4 Chatbot/articles_abstract_pairs/article-abstract-tokenized.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ae3AgsHdfG27",
        "colab_type": "code",
        "outputId": "a54b3538-42cd-43d7-c3e3-ca4ae1d5421b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Dense, Input, RepeatVector, concatenate, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.ops.rnn import dynamic_rnn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AMB1nS5mfK7Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RNN_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "KEEP_PROBABILITY = 0.5\n",
        "OPTIMIZER_TYPE = 'adam'\n",
        "LEARNING_RATE = 0.001\n",
        "EMBEDDING_SIZE = 100\n",
        "\n",
        "data_dir_path = 'drive/My Drive/Assignment 4 Chatbot'\n",
        "data_file = '/articles_abstract_pairs/article-abstract-tokenized.csv'\n",
        "glove_file = '/articles_abstract_pairs/glove.6B.' + str(EMBEDDING_SIZE) + 'd.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-AVYr7nfvO1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VERBOSE = 1\n",
        "NUM_LAYERS = 3\n",
        "MAX_INPUT_SEQ_LENGTH = 500\n",
        "MAX_TARGET_SEQ_LENGTH = 50\n",
        "MAX_INPUT_VOCAB_SIZE = 3000\n",
        "MAX_TARGET_VOCAB_SIZE = 1000\n",
        "NUM_SAMPLES = 5000\n",
        "MAX_DECODER_SEQ_LENGTH = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmA6KDpLfyTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_keras_optimizer():\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, momentum=0.0, decay=0.0, nesterov=False)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.Adagrad(lr=LEARNING_RATE, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        keras_optimizer = keras.optimizers.Adadelta(lr=LEARNING_RATE, rho=0.95, epsilon=None, decay=0.0)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                                                amsgrad=False)\n",
        "\n",
        "    return keras_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qvJoKAnLf1wJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_tf_optimizer(lr):\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.RMSPropOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.AdagradOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        tf_optimizer = tf.train.AdadeltaOptimizer(lr)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    return tf_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lKCB1ge4gAAz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Modeling"
      ]
    },
    {
      "metadata": {
        "id": "2X2xtH8ff5QU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_glove():\n",
        "    with open(data_dir_path + glove_file, 'r') as f:\n",
        "        word2vector = {}\n",
        "        for line in f:\n",
        "            line_ = line.strip()    # Remove white space\n",
        "            words_vec = line_.split()\n",
        "            word2vector[words_vec[0]] = np.array(words_vec[1:], dtype=float)\n",
        "    return word2vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ltvy8o2qgCJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_input_text_glove(texts, max_input_seq_length, unknown_emb, word2em):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = np.zeros(shape=(max_input_seq_length, EMBEDDING_SIZE))\n",
        "        for idx, word in enumerate(line.lower().split(' ')):\n",
        "            if idx >= max_input_seq_length:\n",
        "                break\n",
        "            emb = unknown_emb\n",
        "            if word in word2em:\n",
        "                emb = word2em[word]\n",
        "            x[idx, :] = emb\n",
        "        temp.append(x)\n",
        "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
        "\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7uI1HyQ6iA4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_text(x, y, input_seq_max_length=None, target_seq_max_length=None):\n",
        "    if input_seq_max_length is None:\n",
        "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
        "    if target_seq_max_length is None:\n",
        "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
        "    input_counter = Counter()\n",
        "    target_counter = Counter()\n",
        "    max_input_seq_length = 0\n",
        "    max_target_seq_length = 0\n",
        "\n",
        "    for line in x:\n",
        "        text = [word for word in line.split(' ')]\n",
        "        for i, word in enumerate(text):\n",
        "            if word == '':\n",
        "                del text[i]\n",
        "        seq_length = len(text)\n",
        "        if seq_length > input_seq_max_length:\n",
        "            text = text[0:input_seq_max_length]\n",
        "            seq_length = len(text)\n",
        "        for word in text:\n",
        "            input_counter[word] += 1\n",
        "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
        "\n",
        "    for i, line in enumerate(y):\n",
        "\n",
        "        line2 = 'START ' + str(line) + ' END'\n",
        "        text = [word for word in line2.split(' ')]\n",
        "        for j, word in enumerate(text):\n",
        "            if word == '':\n",
        "                del text[j]\n",
        "        seq_length = len(text)\n",
        "        if seq_length > target_seq_max_length:\n",
        "            text = text[0:target_seq_max_length]\n",
        "            seq_length = len(text)\n",
        "        for word in text:\n",
        "            target_counter[word] += 1\n",
        "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
        "\n",
        "    input_word2idx = dict()\n",
        "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
        "        input_word2idx[word[0]] = idx + 2\n",
        "    input_word2idx['PAD'] = 0\n",
        "    input_word2idx['UNK'] = 1\n",
        "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
        "\n",
        "    target_word2idx = dict()\n",
        "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
        "        target_word2idx[word[0]] = idx + 1\n",
        "    target_word2idx['UNK'] = 0\n",
        "\n",
        "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "\n",
        "    num_input_tokens = len(input_word2idx)\n",
        "    num_target_tokens = len(target_word2idx)\n",
        "\n",
        "    config = dict()\n",
        "    config['input_word2idx'] = input_word2idx\n",
        "    config['input_idx2word'] = input_idx2word\n",
        "    config['target_word2idx'] = target_word2idx\n",
        "    config['target_idx2word'] = target_idx2word\n",
        "    config['num_input_tokens'] = num_input_tokens\n",
        "    config['num_target_tokens'] = num_target_tokens\n",
        "    config['max_input_seq_length'] = max_input_seq_length\n",
        "    config['max_target_seq_length'] = max_target_seq_length\n",
        "\n",
        "    return config\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dG-9gRHxiQ8C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_input_text(texts, input_word2idx, max_input_seq_length):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = []\n",
        "        for word in line.lower().split(' '):\n",
        "            wid = 1\n",
        "            if word in input_word2idx:\n",
        "                wid = input_word2idx[word]\n",
        "            x.append(wid)\n",
        "            if len(x) >= max_input_seq_length:\n",
        "                break\n",
        "        temp.append(x)\n",
        "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
        "\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mre3Pge3iSC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_target_encoding(texts, max_target_seq_length):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = []\n",
        "        line2 = 'START ' + line.lower() + ' END'\n",
        "        for word in line2.split(' '):\n",
        "            x.append(word)\n",
        "            if len(x) >= max_target_seq_length:\n",
        "                break\n",
        "        temp.append(x)\n",
        "\n",
        "    temp = np.array(temp)\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aRu1ABYwgG4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2SeqSummarizer(object):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.num_input_tokens = config['num_input_tokens']\n",
        "        self.max_input_seq_length = config['max_input_seq_length']\n",
        "        self.num_target_tokens = config['num_target_tokens']\n",
        "        self.max_target_seq_length = config['max_target_seq_length']\n",
        "        self.input_word2idx = config['input_word2idx']\n",
        "        self.input_idx2word = config['input_idx2word']\n",
        "        self.target_word2idx = config['target_word2idx']\n",
        "        self.target_idx2word = config['target_idx2word']\n",
        "        self.config = config\n",
        "\n",
        "        self.word2em = dict()\n",
        "        if 'unknown_emb' in config:\n",
        "            self.unknown_emb = config['unknown_emb']\n",
        "        else:\n",
        "            self.unknown_emb = np.random.rand(1, EMBEDDING_SIZE)\n",
        "            config['unknown_emb'] = self.unknown_emb\n",
        "\n",
        "        self.config = config     \n",
        "        \n",
        "        # Define an input sequence and process it.\n",
        "        encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
        "        encoder_embedding = Embedding(input_dim=self.num_input_tokens, output_dim=RNN_SIZE,\n",
        "                                      input_length=self.max_input_seq_length, name='encoder_embedding')\n",
        "        encoder_lstm = LSTM(units=RNN_SIZE, return_state=True, name='encoder_lstm')\n",
        "        \n",
        "        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
        "        # keep the states.\n",
        "        encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "        # Set up the decoder, using `encoder_states` as initial state.\n",
        "        decoder_inputs = Input(shape=(None, self.num_target_tokens), name='decoder_inputs')\n",
        "        \n",
        "        # We set up our decoder to return full output sequences,\n",
        "        # and to return internal states as well. We don't use the return states in the training model, but will use them in inference.\n",
        "        decoder_lstm = LSTM(units=RNN_SIZE, return_state=True, return_sequences=True, name='decoder_lstm')\n",
        "        decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
        "                                                                         initial_state=encoder_states)\n",
        "        decoder_dense = Dense(units=self.num_target_tokens, activation='softmax', name='decoder_dense')\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        \n",
        "        # Define the model that will turn\n",
        "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        optimizer = def_keras_optimizer()\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        \n",
        "        self.model = model\n",
        "\n",
        "        self.encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "        decoder_state_inputs = [Input(shape=(RNN_SIZE,)), Input(shape=(RNN_SIZE,))]\n",
        "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
        "        decoder_states = [state_h, state_c]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        self.decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "    def load_glove(self):\n",
        "        self.word2em = load_glove()\n",
        "\n",
        "    def generate_batch(self, x_samples, y_samples, batch_size):\n",
        "        num_batches = len(x_samples) // batch_size\n",
        "        while True:\n",
        "            for batchIdx in range(0, num_batches):\n",
        "                start = batchIdx * batch_size\n",
        "                end = (batchIdx + 1) * batch_size\n",
        "                encoder_input_data_batch = pad_sequences(x_samples[start:end], self.max_input_seq_length)\n",
        "                decoder_target_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length,\n",
        "                                                            self.num_target_tokens))\n",
        "                decoder_input_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length,\n",
        "                                                           self.num_target_tokens))\n",
        "                for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
        "                    for idx, w in enumerate(target_words):\n",
        "                        w2idx = 0  # default [UNK]\n",
        "                        if w in self.target_word2idx:\n",
        "                            w2idx = self.target_word2idx[w]\n",
        "                        if w2idx != 0:\n",
        "                            decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
        "                            if idx > 0:\n",
        "                                decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
        "                yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n",
        "\n",
        "    def fit(self, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
        "        y_train = transform_target_encoding(y_train, self.max_target_seq_length)\n",
        "        y_test = transform_target_encoding(y_test, self.max_target_seq_length)\n",
        "        \n",
        "        x_train = transform_input_text(x_train, self.input_word2idx, self.max_input_seq_length)\n",
        "        x_test = transform_input_text(x_test, self.input_word2idx, self.max_input_seq_length)\n",
        "        \n",
        "        train_gen = self.generate_batch(x_train, y_train, batch_size)\n",
        "        test_gen = self.generate_batch(x_test, y_test, batch_size)\n",
        "\n",
        "        train_num_batches = len(x_train) // batch_size\n",
        "        test_num_batches = len(x_test) // batch_size\n",
        "\n",
        "        history = self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
        "                                           epochs=epochs,\n",
        "                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches)\n",
        "        return history\n",
        "\n",
        "    def summarize(self, input_text):\n",
        "        input_seq = []\n",
        "        input_wids = []\n",
        "        for word in input_text.lower().split(' '):\n",
        "            idx = 1  # default [UNK]\n",
        "            if word in self.input_word2idx:\n",
        "                idx = self.input_word2idx[word]\n",
        "            input_wids.append(idx)\n",
        "        input_seq.append(input_wids)\n",
        "        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
        "        states_value = self.encoder_model.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
        "        target_seq[0, 0, self.target_word2idx['START']] = 1\n",
        "        target_text = ''\n",
        "        target_text_len = 0\n",
        "        terminated = False\n",
        "        while not terminated:\n",
        "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "            sample_word = self.target_idx2word[sample_token_idx]\n",
        "            target_text_len += 1\n",
        "\n",
        "            if sample_word != 'START' and sample_word != 'END':\n",
        "                target_text += ' ' + sample_word\n",
        "\n",
        "            if sample_word == 'END' or target_text_len >= self.max_target_seq_length:\n",
        "                terminated = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
        "            target_seq[0, 0, sample_token_idx] = 1\n",
        "\n",
        "            states_value = [h, c]\n",
        "        return target_text.strip()\n",
        "\n",
        "    def summarize_glove(self, input_text):\n",
        "        input_seq = np.zeros(shape=(1, self.max_input_seq_length, EMBEDDING_SIZE))\n",
        "        for idx, word in enumerate(input_text.lower().split(' ')):\n",
        "            if idx >= self.max_input_seq_length:\n",
        "                break\n",
        "            emb = self.unknown_emb  # default [UNK]\n",
        "            if word in self.word2em:\n",
        "                emb = self.word2em[word]\n",
        "            input_seq[0, idx, :] = emb\n",
        "        states_value = self.encoder_model.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
        "        target_seq[0, 0, self.target_word2idx['START']] = 1\n",
        "        target_text = ''\n",
        "        target_text_len = 0\n",
        "        terminated = False\n",
        "        while not terminated:\n",
        "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "            sample_word = self.target_idx2word[sample_token_idx]\n",
        "            target_text_len += 1\n",
        "\n",
        "            if sample_word != 'START' and sample_word != 'END':\n",
        "                target_text += ' ' + sample_word\n",
        "\n",
        "            if sample_word == 'END' or target_text_len >= self.max_target_seq_length:\n",
        "                terminated = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
        "            target_seq[0, 0, sample_token_idx] = 1\n",
        "\n",
        "            states_value = [h, c]\n",
        "        return target_text.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "505tk01LhGTk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main_seq2seq():\n",
        "\n",
        "    print('loading csv file ...')\n",
        "    df = pd.read_csv(data_dir_path + data_file)\n",
        "\n",
        "    print('extract configuration from input texts ...')\n",
        "    y = df['abstract']\n",
        "    x = df['article']\n",
        "    config = fit_text(x, y)\n",
        "\n",
        "    print('configuration extracted from input texts ...')\n",
        "\n",
        "    summarizer = Seq2SeqSummarizer(config)\n",
        "    summarizer.load_glove()\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print('training size: ', len(x_train))\n",
        "    print('testing size: ', len(x_test))\n",
        "\n",
        "    print('start fitting ...')\n",
        "    summarizer.fit(x_train, y_train, x_test, y_test, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for i in np.random.permutation(np.arange(len(x)))[0:10]:\n",
        "        _x = x[i]\n",
        "        actual_abstract = y[i]\n",
        "        abstract = summarizer.summarize(_x)\n",
        "       \n",
        "        print('Generated Abstract: ', abstract)\n",
        "        print('Original Abstract: ', actual_abstract)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BegcEevkhiJB",
        "colab_type": "code",
        "outputId": "b1670a5b-ac9c-4715-cb7c-619bff0cc355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "cell_type": "code",
      "source": [
        "main_seq2seq()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading csv file ...\n",
            "extract configuration from input texts ...\n",
            "configuration extracted from input texts ...\n",
            "training size:  34038\n",
            "testing size:  8510\n",
            "start fitting ...\n",
            "(34038,)\n",
            "(8510,)\n",
            "(34038, 213)\n",
            "(8510, 213)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/3\n",
            "531/531 [==============================] - 428s 807ms/step - loss: 1.1563 - acc: 0.0490 - val_loss: 1.1376 - val_acc: 0.0498\n",
            "Epoch 2/3\n",
            "531/531 [==============================] - 567s 1s/step - loss: 1.1215 - acc: 0.0500 - val_loss: 1.1146 - val_acc: 0.0514\n",
            "Epoch 3/3\n",
            "531/531 [==============================] - 558s 1s/step - loss: 1.0931 - acc: 0.0527 - val_loss: 1.0906 - val_acc: 0.0538\n",
            "Generated Abstract:  trump says trump nt . nt . . . . . .\n",
            "Original Abstract:  trump recruits election observers warns potential voter fraud \n",
            "Generated Abstract:  trump nt nt . nt . . . . . . .\n",
            "Original Abstract:  doctors test drones speed delivery lab tests \n",
            "Generated Abstract:  trump says trump nt . nt . . . . . .\n",
            "Original Abstract:  cameron hails eu deal britain special status battle looms \n",
            "Generated Abstract:  trump nt nt . . . . . . . . . .\n",
            "Original Abstract:  bill cosbys defense strategy problem rape victims \n",
            "Generated Abstract:  trump nt nt . nt . . . . . . . .\n",
            "Original Abstract:  itchy eyes sneezing maybe blame allergy neanderthals \n",
            "Generated Abstract:  trump nt nt . . . . . . . . .\n",
            "Original Abstract:  social justice shipping ideology fandom crusade things ugly \n",
            "Generated Abstract:  trump nt nt . nt . . . . . . .\n",
            "Original Abstract:  putin destroyed russias independent press . trump want . \n",
            "Generated Abstract:  trump nt nt . . . . . . . . . .\n",
            "Original Abstract:  female broadcaster set nfl history \n",
            "Generated Abstract:  trump nt nt . nt . . . . . . . .\n",
            "Original Abstract:  dear jeb bush help america stop trump \n",
            "Generated Abstract:  trump nt nt . nt . . . . . . . .\n",
            "Original Abstract:  creamed canned frozen great depression revamped diets \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}