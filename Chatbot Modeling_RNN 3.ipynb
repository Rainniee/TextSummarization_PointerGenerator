{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot RNN Modeling_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainniee/TextSummarization_PointerGenerator/blob/master/Chatbot%20Modeling_RNN%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tKmXKhdj8XsZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ceed852-5baf-451a-85b7-84fc16b924cc"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i8Lj7niZ8oD8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"drive/My Drive/Assignment 4 Chatbot/articles_abstract_pairs/article-abstract-tokenized.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I_PwJPoF8oJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "baebdb9b-f6fb-418e-c40b-5a47899e60d0"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Dense, Input, RepeatVector, concatenate, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.ops.rnn import dynamic_rnn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vHOCZpHL85rZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RNN_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1 #3\n",
        "KEEP_PROBABILITY = 0.5\n",
        "OPTIMIZER_TYPE = 'adam'\n",
        "LEARNING_RATE = 0.001\n",
        "EMBEDDING_SIZE = 100\n",
        "\n",
        "data_dir_path = 'drive/My Drive/Assignment 4 Chatbot'\n",
        "data_file = '/articles_abstract_pairs/article-abstract-tokenized.csv'\n",
        "glove_file = '/articles_abstract_pairs/glove.6B.' + str(EMBEDDING_SIZE) + 'd.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sEJOWyeT88mo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VERBOSE = 1\n",
        "NUM_LAYERS = 3\n",
        "MAX_INPUT_SEQ_LENGTH = 500\n",
        "MAX_TARGET_SEQ_LENGTH = 50\n",
        "MAX_INPUT_VOCAB_SIZE = 3000\n",
        "MAX_TARGET_VOCAB_SIZE = 1000\n",
        "NUM_SAMPLES = 5000\n",
        "MAX_DECODER_SEQ_LENGTH = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cd6NbN3u9Ao1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_keras_optimizer():\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, momentum=0.0, decay=0.0, nesterov=False)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.Adagrad(lr=LEARNING_RATE, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        keras_optimizer = keras.optimizers.Adadelta(lr=LEARNING_RATE, rho=0.95, epsilon=None, decay=0.0)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                                                amsgrad=False)\n",
        "\n",
        "    return keras_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5eDkbeOT9DZF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_tf_optimizer(lr):\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.RMSPropOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.AdagradOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        tf_optimizer = tf.train.AdadeltaOptimizer(lr)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    return tf_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2mQ2mW09Nnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN Modeling"
      ]
    },
    {
      "metadata": {
        "id": "8VXkWUbo9L8C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_text(x, y, input_seq_max_length=None, target_seq_max_length=None):\n",
        "    if input_seq_max_length is None:\n",
        "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
        "    if target_seq_max_length is None:\n",
        "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
        "    input_counter = Counter()\n",
        "    target_counter = Counter()\n",
        "    max_input_seq_length = 0\n",
        "    max_target_seq_length = 0\n",
        "\n",
        "    for line in x:\n",
        "        text = [word for word in line.split(' ')]\n",
        "        for i, word in enumerate(text):\n",
        "            if word == '':\n",
        "                del text[i]\n",
        "        seq_length = len(text)\n",
        "        if seq_length > input_seq_max_length:\n",
        "            text = text[0:input_seq_max_length]\n",
        "            seq_length = len(text)\n",
        "        for word in text:\n",
        "            input_counter[word] += 1\n",
        "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
        "\n",
        "    for i, line in enumerate(y):\n",
        "\n",
        "        line2 = 'START ' + str(line) + ' END'\n",
        "        text = [word for word in line2.split(' ')]\n",
        "        for j, word in enumerate(text):\n",
        "            if word == '':\n",
        "                del text[j]\n",
        "        seq_length = len(text)\n",
        "        if seq_length > target_seq_max_length:\n",
        "            text = text[0:target_seq_max_length]\n",
        "            seq_length = len(text)\n",
        "        for word in text:\n",
        "            target_counter[word] += 1\n",
        "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
        "\n",
        "    input_word2idx = dict()\n",
        "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
        "        input_word2idx[word[0]] = idx + 2\n",
        "    input_word2idx['PAD'] = 0\n",
        "    input_word2idx['UNK'] = 1\n",
        "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
        "\n",
        "    target_word2idx = dict()\n",
        "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
        "        target_word2idx[word[0]] = idx + 1\n",
        "    target_word2idx['UNK'] = 0\n",
        "\n",
        "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "\n",
        "    num_input_tokens = len(input_word2idx)\n",
        "    num_target_tokens = len(target_word2idx)\n",
        "\n",
        "    config = dict()\n",
        "    config['input_word2idx'] = input_word2idx\n",
        "    config['input_idx2word'] = input_idx2word\n",
        "    config['target_word2idx'] = target_word2idx\n",
        "    config['target_idx2word'] = target_idx2word\n",
        "    config['num_input_tokens'] = num_input_tokens\n",
        "    config['num_target_tokens'] = num_target_tokens\n",
        "    config['max_input_seq_length'] = max_input_seq_length\n",
        "    config['max_target_seq_length'] = max_target_seq_length\n",
        "\n",
        "    return config\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sXofLGNu9b9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_input_text(texts, input_word2idx, max_input_seq_length):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = []\n",
        "        for word in line.lower().split(' '):\n",
        "            wid = 1\n",
        "            if word in input_word2idx:\n",
        "                wid = input_word2idx[word]\n",
        "            x.append(wid)\n",
        "            if len(x) >= max_input_seq_length:\n",
        "                break\n",
        "        temp.append(x)\n",
        "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
        "\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwR_wiAX9cqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_target_encoding(texts, max_target_seq_length):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = []\n",
        "        line2 = 'START ' + line.lower() + ' END'\n",
        "        for word in line2.split(' '):\n",
        "            x.append(word)\n",
        "            if len(x) >= max_target_seq_length:\n",
        "                break\n",
        "        temp.append(x)\n",
        "\n",
        "    temp = np.array(temp)\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7mB7O-fG9e7K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RecursiveRNN(object):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.num_input_tokens = config['num_input_tokens']\n",
        "        self.max_input_seq_length = config['max_input_seq_length']\n",
        "        self.num_target_tokens = config['num_target_tokens']\n",
        "        self.max_target_seq_length = config['max_target_seq_length']\n",
        "        self.input_word2idx = config['input_word2idx']\n",
        "        self.input_idx2word = config['input_idx2word']\n",
        "        self.target_word2idx = config['target_word2idx']\n",
        "        self.target_idx2word = config['target_idx2word']\n",
        "        self.config = config\n",
        "\n",
        "        # article input model\n",
        "        inputs1 = Input(shape=(self.max_input_seq_length,))\n",
        "        article1 = Embedding(self.num_input_tokens, 128)(inputs1)\n",
        "        article2 = LSTM(128)(article1)\n",
        "        article3 = RepeatVector(128)(article2)\n",
        "        # summary input model\n",
        "        inputs2 = Input(shape=(self.max_target_seq_length,))\n",
        "        summ1 = Embedding(self.num_target_tokens, 128)(inputs2)\n",
        "        summ2 = LSTM(128)(summ1)\n",
        "        summ3 = RepeatVector(128)(summ2)\n",
        "        # decoder model\n",
        "        decoder1 = concatenate([article3, summ3])\n",
        "        decoder2 = LSTM(128)(decoder1)\n",
        "        outputs = Dense(self.num_target_tokens, activation='softmax')(decoder2)\n",
        "\n",
        "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "        optimizer = def_keras_optimizer()\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "    def generate_batch(self, x_samples, y_samples, batch_size):\n",
        "        encoder_input_data_batch = []\n",
        "        decoder_input_data_batch = []\n",
        "        decoder_target_data_batch = []\n",
        "        line_idx = 0\n",
        "        while True:\n",
        "            for recordIdx in range(0, len(x_samples)):\n",
        "                target_words = y_samples[recordIdx]\n",
        "                x = x_samples[recordIdx]\n",
        "                decoder_input_line = []\n",
        "\n",
        "                for idx in range(0, len(target_words) - 1):\n",
        "                    w2idx = 0  # default [UNK]\n",
        "                    w = target_words[idx]\n",
        "                    if w in self.target_word2idx:\n",
        "                        w2idx = self.target_word2idx[w]\n",
        "                    decoder_input_line = decoder_input_line + [w2idx]\n",
        "                    decoder_target_label = np.zeros(self.num_target_tokens)\n",
        "                    w2idx_next = 0\n",
        "                    if target_words[idx + 1] in self.target_word2idx:\n",
        "                        w2idx_next = self.target_word2idx[target_words[idx + 1]]\n",
        "                    if w2idx_next != 0:\n",
        "                        decoder_target_label[w2idx_next] = 1\n",
        "                    decoder_input_data_batch.append(decoder_input_line)\n",
        "                    encoder_input_data_batch.append(x)\n",
        "                    decoder_target_data_batch.append(decoder_target_label)\n",
        "\n",
        "                    line_idx += 1\n",
        "                    if line_idx >= batch_size:\n",
        "                        yield [pad_sequences(encoder_input_data_batch, self.max_input_seq_length),\n",
        "                               pad_sequences(decoder_input_data_batch,\n",
        "                                             self.max_target_seq_length)], np.array(decoder_target_data_batch)\n",
        "                        \n",
        "                        encoder_input_data_batch = []\n",
        "                        decoder_input_data_batch = []\n",
        "                        decoder_target_data_batch = []\n",
        "\n",
        "    def fit(self, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
        "\n",
        "        y_train = transform_target_encoding(y_train, self.max_target_seq_length)\n",
        "        y_test = transform_target_encoding(y_test, self.max_target_seq_length)\n",
        "\n",
        "        x_train = transform_input_text(x_train, self.input_word2idx, self.max_input_seq_length)\n",
        "        x_test = transform_input_text(x_test, self.input_word2idx, self.max_input_seq_length)\n",
        "\n",
        "        train_gen = self.generate_batch(x_train, y_train, batch_size)\n",
        "        test_gen = self.generate_batch(x_test, y_test, batch_size)\n",
        "\n",
        "        total_training_samples = sum([len(target_text) - 1 for target_text in y_train])\n",
        "        total_testing_samples = sum([len(target_text) - 1 for target_text in y_test])\n",
        "        train_num_batches = total_training_samples // batch_size\n",
        "        test_num_batches = total_testing_samples // batch_size\n",
        "\n",
        "        self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches, epochs=epochs, verbose=VERBOSE,\n",
        "                                 validation_data=test_gen, validation_steps=test_num_batches)\n",
        "\n",
        "    def summarize(self, input_text):\n",
        "        input_seq = []\n",
        "        input_wids = []\n",
        "        for word in input_text.lower().split(' '):\n",
        "            idx = 1  # default [UNK]\n",
        "            if word in self.input_word2idx:\n",
        "                idx = self.input_word2idx[word]\n",
        "            input_wids.append(idx)\n",
        "        input_seq.append(input_wids)\n",
        "        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
        "        start_token = self.target_word2idx['START']\n",
        "        wid_list = [start_token]\n",
        "        sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n",
        "        \n",
        "        terminated = False\n",
        "\n",
        "        target_text = ''\n",
        "\n",
        "        while not terminated:\n",
        "            output_tokens = self.model.predict([input_seq, sum_input_seq])\n",
        "            sample_token_idx = np.argmax(output_tokens[0, :])\n",
        "            sample_word = self.target_idx2word[sample_token_idx]\n",
        "            wid_list = wid_list + [sample_token_idx]\n",
        "\n",
        "            if sample_word != 'START' and sample_word != 'END':\n",
        "                target_text += ' ' + sample_word\n",
        "\n",
        "            if sample_word == 'END' or len(wid_list) >= self.max_target_seq_length:\n",
        "                terminated = True\n",
        "            else:\n",
        "                sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n",
        "                \n",
        "        return target_text.strip()\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7nk8MLLl__8q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main_rnn():\n",
        "    print('loading csv file ...')\n",
        "    df = pd.read_csv(data_dir_path + data_file)\n",
        "\n",
        "    print('extract configuration from input texts ...')\n",
        "    y = df['abstract']\n",
        "    x = df['article']\n",
        "    config = fit_text(x, y)\n",
        "\n",
        "    print('configuration extracted from input texts ...')\n",
        "\n",
        "    summarizer = RecursiveRNN(config)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print('demo size: ', len(x_train))\n",
        "    print('testing size: ', len(x_test))\n",
        "\n",
        "    print('start fitting ...')\n",
        "    summarizer.fit(x_train, y_train, x_test, y_test, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    print('start predicting ...')\n",
        "    for i in np.random.permutation(np.arange(len(x)))[0:10]:\n",
        "        x = x[i]\n",
        "        actual_abstract = y[i]\n",
        "        abstract = summarizer.summarize(x)\n",
        "        print('Article: ', x)\n",
        "        print('Generated Abstract: ', abstract)\n",
        "        print('Original Abstract: ', actual_abstract)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3L5VudBQAXxB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "outputId": "5a8ad489-7d99-4af2-e4d8-9572c97342f4"
      },
      "cell_type": "code",
      "source": [
        "main_rnn()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading csv file ...\n",
            "extract configuration from input texts ...\n",
            "configuration extracted from input texts ...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "demo size:  34038\n",
            "testing size:  8510\n",
            "start fitting ...\n",
            "(34038,)\n",
            "(8510,)\n",
            "(34038, 213)\n",
            "(8510, 213)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "4960/4960 [==============================] - 5107s 1s/step - loss: 3.3469 - acc: 0.1073 - val_loss: 3.4560 - val_acc: 0.1086\n",
            "start predicting ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-da1f352b032b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-dac3561f3289>\u001b[0m in \u001b[0;36mmain_rnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mactual_abstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Article: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generated Abstract: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a73b7d59ac31>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mstart_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_word2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'START'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mwid_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstart_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mNUM_ALGORITHME\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0msum_input_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwid_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_target_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NUM_ALGORITHME' is not defined"
          ]
        }
      ]
    }
  ]
}