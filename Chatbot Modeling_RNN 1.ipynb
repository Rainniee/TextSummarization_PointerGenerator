{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot RNN Modeling 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainniee/TextSummarization_PointerGenerator/blob/master/Chatbot%20Modeling_RNN%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dpbblTA9rqJp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Tokenied Dataset"
      ]
    },
    {
      "metadata": {
        "id": "anBW8fm-hwh2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Followed Adhira's binsss scripts, which generated story files and tokenized then wrote to bin\n",
        "   I then converted the bin files to txt and made train.txt to article_abstract_tokenized.csv file\n",
        "   The bin to txt scripts can be found in Convert BinaryFile to Text'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cx8AiFh-3ebR",
        "colab_type": "code",
        "outputId": "9200357a-c833-49e3-d03a-3ed31a9f4f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f-Dw_ISN1T82",
        "colab_type": "code",
        "outputId": "66578d45-d959-47c2-fd7f-f2422b1d204c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"drive/My Drive/Assignment 4 Chatbot/articles_abstract_pairs/article-abstract-tokenized.csv\")\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>shakespeares folios sell auction 25 m</td>\n",
              "      <td>copies william shakespeares books dubbed holy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>grandmothers death saved life debt</td>\n",
              "      <td>debt 20 000 source college credit cards estima...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>feared life lacked meaning . cancer pushed find</td>\n",
              "      <td>late . drunk nearing 35th birthday past dank c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>texas man serving life sentence innocent doubl...</td>\n",
              "      <td>central texas man serving life sentence double...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dads reagan protests inspire stand donald trump</td>\n",
              "      <td>battling depression sleeplessness thinking fig...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract  \\\n",
              "0             shakespeares folios sell auction 25 m    \n",
              "1                grandmothers death saved life debt    \n",
              "2   feared life lacked meaning . cancer pushed find    \n",
              "3  texas man serving life sentence innocent doubl...   \n",
              "4   dads reagan protests inspire stand donald trump    \n",
              "\n",
              "                                             article  \n",
              "0  copies william shakespeares books dubbed holy ...  \n",
              "1  debt 20 000 source college credit cards estima...  \n",
              "2  late . drunk nearing 35th birthday past dank c...  \n",
              "3  central texas man serving life sentence double...  \n",
              "4  battling depression sleeplessness thinking fig...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "teuWkE5HmGKb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set Parameters"
      ]
    },
    {
      "metadata": {
        "id": "paqGy9em4ATs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Dense, Input, RepeatVector, concatenate, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.ops.rnn import dynamic_rnn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KIR-iZV04hxN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RNN_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "KEEP_PROBABILITY = 0.5\n",
        "OPTIMIZER_TYPE = 'adam'\n",
        "LEARNING_RATE = 0.001\n",
        "EMBEDDING_SIZE = 100\n",
        "\n",
        "data_dir_path = 'drive/My Drive/Assignment 4 Chatbot'\n",
        "data_file = '/articles_abstract_pairs/article-abstract-tokenized.csv'\n",
        "glove_file = '/articles_abstract_pairs/glove.6B.' + str(EMBEDDING_SIZE) + 'd.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGMtHroeioZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VERBOSE = 1\n",
        "NUM_LAYERS = 3\n",
        "MAX_INPUT_SEQ_LENGTH = 500\n",
        "MAX_TARGET_SEQ_LENGTH = 50\n",
        "MAX_INPUT_VOCAB_SIZE = 3000\n",
        "MAX_TARGET_VOCAB_SIZE = 1000\n",
        "NUM_SAMPLES = 5000\n",
        "MAX_DECODER_SEQ_LENGTH = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MrV2cEorizDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_keras_optimizer():\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, momentum=0.0, decay=0.0, nesterov=False)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.Adagrad(lr=LEARNING_RATE, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        keras_optimizer = keras.optimizers.Adadelta(lr=LEARNING_RATE, rho=0.95, epsilon=None, decay=0.0)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                                                amsgrad=False)\n",
        "\n",
        "    return keras_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uR9qOZJizGG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_tf_optimizer(lr):\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.RMSPropOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.AdagradOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        tf_optimizer = tf.train.AdadeltaOptimizer(lr)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    return tf_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVrijqnRoriA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Modeling"
      ]
    },
    {
      "metadata": {
        "id": "hzcNWMSlotH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_text(x, y, input_seq_max_length=None, target_seq_max_length=None):\n",
        "    if input_seq_max_length is None:\n",
        "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
        "    if target_seq_max_length is None:\n",
        "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
        "    input_counter = Counter()\n",
        "    target_counter = Counter()\n",
        "    max_input_seq_length = 0\n",
        "    max_target_seq_length = 0\n",
        "\n",
        "    for line in x:\n",
        "        text = [word for word in line.split(' ')]\n",
        "        for i, word in enumerate(text):\n",
        "            if word == '':\n",
        "                del text[i]\n",
        "        seq_length = len(text)\n",
        "        if seq_length > input_seq_max_length:\n",
        "            text = text[0:input_seq_max_length]\n",
        "            seq_length = len(text)\n",
        "        for word in text:\n",
        "            input_counter[word] += 1\n",
        "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
        "\n",
        "    for i, line in enumerate(y):\n",
        "\n",
        "        line2 = 'START ' + str(line) + ' END'\n",
        "        text = [word for word in line2.split(' ')]\n",
        "        for j, word in enumerate(text):\n",
        "            if word == '':\n",
        "                del text[j]\n",
        "        seq_length = len(text)\n",
        "        if seq_length > target_seq_max_length:\n",
        "            text = text[0:target_seq_max_length]\n",
        "            seq_length = len(text)\n",
        "        for word in text:\n",
        "            target_counter[word] += 1\n",
        "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
        "\n",
        "    input_word2idx = dict()\n",
        "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
        "        input_word2idx[word[0]] = idx + 2\n",
        "    input_word2idx['PAD'] = 0\n",
        "    input_word2idx['UNK'] = 1\n",
        "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
        "\n",
        "    target_word2idx = dict()\n",
        "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
        "        target_word2idx[word[0]] = idx + 1\n",
        "    target_word2idx['UNK'] = 0\n",
        "\n",
        "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "\n",
        "    num_input_tokens = len(input_word2idx)\n",
        "    num_target_tokens = len(target_word2idx)\n",
        "\n",
        "    config = dict()\n",
        "    config['input_word2idx'] = input_word2idx\n",
        "    config['input_idx2word'] = input_idx2word\n",
        "    config['target_word2idx'] = target_word2idx\n",
        "    config['target_idx2word'] = target_idx2word\n",
        "    config['num_input_tokens'] = num_input_tokens\n",
        "    config['num_target_tokens'] = num_target_tokens\n",
        "    config['max_input_seq_length'] = max_input_seq_length\n",
        "    config['max_target_seq_length'] = max_target_seq_length\n",
        "\n",
        "    return config\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzjKFjCio6PE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_input_text(texts, input_word2idx, max_input_seq_length):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = []\n",
        "        for word in line.lower().split(' '):\n",
        "            wid = 1\n",
        "            if word in input_word2idx:\n",
        "                wid = input_word2idx[word]\n",
        "            x.append(wid)\n",
        "            if len(x) >= max_input_seq_length:\n",
        "                break\n",
        "        temp.append(x)\n",
        "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
        "\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2Mnxtrko-lG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_target_encoding(texts, max_target_seq_length):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = []\n",
        "        line2 = 'START ' + line.lower() + ' END'\n",
        "        for word in line2.split(' '):\n",
        "            x.append(word)\n",
        "            if len(x) >= max_target_seq_length:\n",
        "                break\n",
        "        temp.append(x)\n",
        "\n",
        "    temp = np.array(temp)\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2k_-BNkpCrK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RecursiveRNN(object):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.num_input_tokens = config['num_input_tokens']\n",
        "        self.max_input_seq_length = config['max_input_seq_length']\n",
        "        self.num_target_tokens = config['num_target_tokens']\n",
        "        self.max_target_seq_length = config['max_target_seq_length']\n",
        "        self.input_word2idx = config['input_word2idx']\n",
        "        self.input_idx2word = config['input_idx2word']\n",
        "        self.target_word2idx = config['target_word2idx']\n",
        "        self.target_idx2word = config['target_idx2word']\n",
        "        self.config = config\n",
        "\n",
        "        inputs1 = Input(shape=(self.max_input_seq_length,))\n",
        "        am1 = Embedding(self.num_input_tokens, 128)(inputs1)\n",
        "        am2 = LSTM(128)(am1)\n",
        "\n",
        "        inputs2 = Input(shape=(self.max_target_seq_length,))\n",
        "        sm1 = Embedding(self.num_target_tokens, 128)(inputs2)\n",
        "        sm2 = LSTM(128)(sm1)\n",
        "\n",
        "        decoder1 = concatenate([am2, sm2])\n",
        "        outputs = Dense(self.num_target_tokens, activation='softmax')(decoder1)      \n",
        "\n",
        "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "        optimizer = def_keras_optimizer()\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "    def generate_batch(self, x_samples, y_samples, batch_size):\n",
        "        encoder_input_data_batch = []\n",
        "        decoder_input_data_batch = []\n",
        "        decoder_target_data_batch = []\n",
        "        line_idx = 0\n",
        "        while True:\n",
        "            for recordIdx in range(0, len(x_samples)):\n",
        "                target_words = y_samples[recordIdx]\n",
        "                x = x_samples[recordIdx]\n",
        "                decoder_input_line = []\n",
        "\n",
        "                for idx in range(0, len(target_words) - 1):\n",
        "                    w2idx = 0  # default [UNK]\n",
        "                    w = target_words[idx]\n",
        "                    if w in self.target_word2idx:\n",
        "                        w2idx = self.target_word2idx[w]\n",
        "                    decoder_input_line = decoder_input_line + [w2idx]\n",
        "                    decoder_target_label = np.zeros(self.num_target_tokens)\n",
        "                    w2idx_next = 0\n",
        "                    if target_words[idx + 1] in self.target_word2idx:\n",
        "                        w2idx_next = self.target_word2idx[target_words[idx + 1]]\n",
        "                    if w2idx_next != 0:\n",
        "                        decoder_target_label[w2idx_next] = 1\n",
        "                    decoder_input_data_batch.append(decoder_input_line)\n",
        "                    encoder_input_data_batch.append(x)\n",
        "                    decoder_target_data_batch.append(decoder_target_label)\n",
        "\n",
        "                    line_idx += 1\n",
        "                    if line_idx >= batch_size:\n",
        "                        yield [pad_sequences(encoder_input_data_batch, self.max_input_seq_length),\n",
        "                               pad_sequences(decoder_input_data_batch,\n",
        "                                             self.max_target_seq_length)], np.array(decoder_target_data_batch)                   \n",
        "                        line_idx = 0\n",
        "                        encoder_input_data_batch = []\n",
        "                        decoder_input_data_batch = []\n",
        "                        decoder_target_data_batch = []\n",
        "\n",
        "    def fit(self, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
        "\n",
        "        y_train = transform_target_encoding(y_train, self.max_target_seq_length)\n",
        "        y_test = transform_target_encoding(y_test, self.max_target_seq_length)\n",
        "\n",
        "        x_train = transform_input_text(x_train, self.input_word2idx, self.max_input_seq_length)\n",
        "        x_test = transform_input_text(x_test, self.input_word2idx, self.max_input_seq_length)\n",
        "\n",
        "        train_gen = self.generate_batch(x_train, y_train, batch_size)\n",
        "        test_gen = self.generate_batch(x_test, y_test, batch_size)\n",
        "\n",
        "        total_training_samples = sum([len(target_text) - 1 for target_text in y_train])\n",
        "        total_testing_samples = sum([len(target_text) - 1 for target_text in y_test])\n",
        "        train_num_batches = total_training_samples // batch_size\n",
        "        test_num_batches = total_testing_samples // batch_size\n",
        "\n",
        "        self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches, epochs=epochs, verbose=VERBOSE,\n",
        "                                 validation_data=test_gen, validation_steps=test_num_batches)\n",
        "\n",
        "    def summarize(self, input_text):\n",
        "        input_seq = []\n",
        "        input_wids = []\n",
        "        for word in input_text.lower().split(' '):\n",
        "            idx = 1  # default [UNK]\n",
        "            if word in self.input_word2idx:\n",
        "                idx = self.input_word2idx[word]\n",
        "            input_wids.append(idx)\n",
        "        input_seq.append(input_wids)\n",
        "        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
        "        start_token = self.target_word2idx['START']\n",
        "        wid_list = [start_token]\n",
        "        sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n",
        "        terminated = False\n",
        "\n",
        "        target_text = ''\n",
        "\n",
        "        while not terminated:\n",
        "            output_tokens = self.model.predict([input_seq, sum_input_seq])\n",
        "            sample_token_idx = np.argmax(output_tokens[0, :])\n",
        "            sample_word = self.target_idx2word[sample_token_idx]\n",
        "            wid_list = wid_list + [sample_token_idx]\n",
        "\n",
        "            if sample_word != 'START' and sample_word != 'END':\n",
        "                target_text += ' ' + sample_word\n",
        "\n",
        "            if sample_word == 'END' or len(wid_list) >= self.max_target_seq_length:\n",
        "                terminated = True\n",
        "            else:\n",
        "                sum_input_seq = pad_sequences([wid_list], self.max_target_seq_length)\n",
        "                \n",
        "        return target_text.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgZXbenlpWlH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main_rnn():\n",
        "    print('loading csv file ...')\n",
        "    df = pd.read_csv(data_dir_path + data_file)\n",
        "\n",
        "    print('extract configuration from input texts ...')\n",
        "    y = df['abstract']\n",
        "    x = df['article']\n",
        "    config = fit_text(x, y)\n",
        "\n",
        "    print('configuration extracted from input texts ...')\n",
        "\n",
        "    summarizer = RecursiveRNN(config)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print('training size: ', len(x_train))\n",
        "    print('testing size: ', len(x_test))\n",
        "\n",
        "    print('start fitting ...')\n",
        "    summarizer.fit(x_train, y_train, x_test, y_test, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    print('start predicting ...')\n",
        "    for i in np.random.permutation(np.arange(len(x)))[0:10]:\n",
        "        x = x[i]\n",
        "        actual_abstract = y[i]\n",
        "        abstract = summarizer.summarize(x)\n",
        "        print('Article: ', x)\n",
        "        print('Generated Abstract: ', abstract)\n",
        "        print('Original Abstract: ', actual_abstract)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LWVq3RNopara",
        "colab_type": "code",
        "outputId": "b52c1389-1923-4a4a-dd7a-d6f5b203d613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "main_rnn()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading csv file ...\n",
            "extract configuration from input texts ...\n",
            "configuration extracted from input texts ...\n",
            "training size:  34038\n",
            "testing size:  8510\n",
            "start fitting ...\n",
            "(34038,)\n",
            "(8510,)\n",
            "(34038, 213)\n",
            "(8510, 213)\n",
            "Epoch 1/5\n",
            "4960/4960 [==============================] - 2846s 574ms/step - loss: 2.5599 - acc: 0.1294 - val_loss: 2.4402 - val_acc: 0.1397\n",
            "Epoch 2/5\n",
            "4960/4960 [==============================] - 2793s 563ms/step - loss: 2.3679 - acc: 0.1443 - val_loss: 2.3576 - val_acc: 0.1471\n",
            "Epoch 3/5\n",
            "4960/4960 [==============================] - 2673s 539ms/step - loss: 2.2673 - acc: 0.1505 - val_loss: 2.3356 - val_acc: 0.1495\n",
            "Epoch 4/5\n",
            "4960/4960 [==============================] - 2780s 561ms/step - loss: 2.1817 - acc: 0.1549 - val_loss: 2.3456 - val_acc: 0.1494\n",
            "Epoch 5/5\n",
            "2053/4960 [===========>..................] - ETA: 24:33 - loss: 2.1273 - acc: 0.1586"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "60kG8uYFbfyj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}