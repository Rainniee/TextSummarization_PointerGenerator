{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "binsss.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adhira-Deogade/TextSummarization_PointerGenerator/blob/master/binsss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "alpCICD79YtG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f9a87b8-c7d0-4835-a40c-94ab77ff4b78"
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rWBbxX5w9crC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHwoumi39fBH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0TcuHtH9kUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0522d9f9-47b1-4ed0-99d7-ed240f5d4977"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34mDrive\u001b[0m/  \u001b[01;34mMy\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bkT6olCM9nWs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba72609d-922e-4948-ce32-5a03d93d5eb7"
      },
      "cell_type": "code",
      "source": [
        "cd Drive/TS/TS/stanford-corenlp-full-2018-10-05/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Drive/TS/TS/stanford-corenlp-full-2018-10-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QkqZvuns9qyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "3ea62022-f419-44de-cb11-6b89eee558c0"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " build.xml                                  LICENSE.txt\n",
            " \u001b[0m\u001b[01;34mcnn_stories_tokenized\u001b[0m/                     make_datafiles3.py\n",
            " corenlp.sh                                 Makefile\n",
            " CoreNLP-to-HTML.xsl                        \u001b[01;34mpatterns\u001b[0m/\n",
            " \u001b[01;34mdm_stories_tokenized\u001b[0m/                      pom-java-11.xml\n",
            " ejml-0.23.jar                              pom.xml\n",
            " ejml-0.23-src.zip                          protobuf.jar\n",
            " \u001b[01;34mfinished_files\u001b[0m/                            README.txt\n",
            " input.txt                                  RESOURCE-LICENSES\n",
            " input.txt.xml                              SemgrexDemo.java\n",
            " javax.activation-api-1.2.0.jar             ShiftReduceDemo.java\n",
            " javax.activation-api-1.2.0-sources.jar     slf4j-api.jar\n",
            " javax.json-api-1.0-sources.jar             slf4j-simple.jar\n",
            " javax.json.jar                             stanford-corenlp-3.9.2.jar\n",
            " jaxb-api-2.4.0-b180830.0359.jar            stanford-corenlp-3.9.2-javadoc.jar\n",
            " jaxb-api-2.4.0-b180830.0359-sources.jar    stanford-corenlp-3.9.2-models.jar\n",
            " jaxb-core-2.3.0.1.jar                      stanford-corenlp-3.9.2-sources.jar\n",
            " jaxb-core-2.3.0.1-sources.jar              StanfordCoreNlpDemo.java\n",
            " jaxb-impl-2.4.0-b180830.0438.jar           StanfordDependenciesManual.pdf\n",
            " jaxb-impl-2.4.0-b180830.0438-sources.jar   \u001b[01;34msutime\u001b[0m/\n",
            " joda-time-2.9-sources.jar                  \u001b[01;34mtokensregex\u001b[0m/\n",
            " joda-time.jar                             \u001b[01;34m'url_lists (ac17bbc3)'\u001b[0m/\n",
            " jollyday-0.4.9-sources.jar                 xom-1.2.10-src.jar\n",
            " jollyday.jar                               xom.jar\n",
            " LIBRARY-LICENSES\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AofFwOhv9ruN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import os, os.path, random\n",
        "import subprocess\n",
        "import hashlib\n",
        "import struct\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "from tensorflow.core.example import example_pb2\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "\n",
        "\n",
        "nlp = StanfordCoreNLP(r'/content/Drive/TS/TS/stanford-corenlp-full-2018-10-05')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXyYQ1SV9zFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_data(stories_dir,train=0.5,test=0.3):\n",
        "    val = 1-(train+test)\n",
        "    \n",
        "    all_files = os.listdir(stories_dir)\n",
        "    total = len(all_files)\n",
        "    shuffled = all_files[:]\n",
        "    random.shuffle(shuffled)\n",
        "    \n",
        "    train_stories = int(round(train*total))\n",
        "    test_stories = int(round(test*total))\n",
        "    val_stories = int(round(val*total))\n",
        "\n",
        "    train_data = shuffled[:train_stories]\n",
        "    test_data = shuffled[train_stories:train_stories+test_stories]\n",
        "    val_data = shuffled[train_stories+test_stories:train_stories+test_stories+val_stories]\n",
        "    return train_data,test_data,val_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8wNYamgr92mU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_stories(stories_dir, tokenized_stories_dir):\n",
        "    \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
        "    print(f\"Preparing to tokenize {stories_dir} to {tokenized_stories_dir}...\") \n",
        "    stories = os.listdir(stories_dir)\n",
        "    # make IO list file\n",
        "    print(\"Making list of files to tokenize...\")\n",
        "    with open(\"mapping.txt\", \"w\") as f:\n",
        "        for s in stories:\n",
        "            f.write(\"%s \\t %s\\n\" % (os.path.join(stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
        "    \n",
        "    command = ['java','-cp', \"*\", '-Xmx2g', 'edu.stanford.nlp.process.PTBTokenizer', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
        "    print(f\"Tokenizing {len(stories)} files in {stories_dir} and saving in {tokenized_stories_dir}...\")\n",
        "    subprocess.call(command)\n",
        "    \n",
        "    print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
        "    \n",
        "    os.remove(\"mapping.txt\")\n",
        "\n",
        "    # Check that the tokenized stories directory contains the same number of files as the original directory\n",
        "    num_orig = len(os.listdir(stories_dir))\n",
        "    num_tokenized = len(os.listdir(tokenized_stories_dir))\n",
        "    print (f\"Successfully finished tokenizing {stories_dir} to {tokenized_stories_dir}.\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i7KiuEI2Ar86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dm_single_close_quote = u'\\u2019' # unicode\n",
        "dm_double_close_quote = u'\\u201d'\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
        "SENTENCE_START = '<s>'\n",
        "SENTENCE_END = '</s>'\n",
        "VOCAB_SIZE = 2000\n",
        "CHUNK_SIZE = 10\n",
        "\n",
        "\n",
        "def read_text_file(text_file):\n",
        "    lines = []\n",
        "    with open(text_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            lines.append(line.strip())\n",
        "    return lines\n",
        "\n",
        "def fix_missing_period(line):\n",
        "    \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
        "    if \"@highlight\" in line: return line\n",
        "    if line==\"\": return line\n",
        "    if line[-1] in END_TOKENS: return line\n",
        "    return line + \" .\"\n",
        "\n",
        "\n",
        "def get_art_abs(story_file):\n",
        "    lines = read_text_file(story_file)\n",
        "\n",
        "    # Lowercase everything\n",
        "    lines = [line.lower() for line in lines]\n",
        "\n",
        "    # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n",
        "    lines = [fix_missing_period(line) for line in lines]\n",
        "\n",
        "    # Separate out article and abstract sentences\n",
        "    article_lines = []\n",
        "    highlights = []\n",
        "    next_is_highlight = False\n",
        "   \n",
        "    for idx,line in enumerate(lines):\n",
        "        if line == \"\":\n",
        "            continue # empty line\n",
        "        elif line.startswith(\"@highlight\"):\n",
        "            next_is_highlight = True\n",
        "        elif next_is_highlight:\n",
        "            highlights.append(line)\n",
        "        else:\n",
        "            article_lines.append(line)\n",
        "\n",
        "        # Make article into a single string\n",
        "        article = ' '.join(article_lines)\n",
        "\n",
        "        # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
        "        abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n",
        "\n",
        "    return article, abstract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-nNS6_P1Ar6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7EG5uh1Ar4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h_To6LsNAr1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FYyH7fNnAryJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NPHGP96T-Yb7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_to_bin(file_names,story_dir,out_file,finished_files_dir, makevocab=False):\n",
        "    \"\"\"Reads the tokenized .story files corresponding to the names listed in the url_file and writes them to an out_file.\"\"\"\n",
        "    \n",
        "    story_fnames = file_names\n",
        "    num_stories = len(story_fnames)\n",
        "\n",
        "    if makevocab:\n",
        "        vocab_counter = collections.Counter()\n",
        "\n",
        "    with open(out_file, 'wb') as writer:\n",
        "        for idx,s in enumerate(story_fnames):\n",
        "            if idx % 1000 == 0:\n",
        "                print (f\"Writing story {idx} of {num_stories}; {float(idx)*100.0/float(num_stories)} percent done\")\n",
        "\n",
        "            story_file = os.path.join(story_dir, s)\n",
        "            \n",
        "            # Get the strings to write to .bin file\n",
        "            article, abstract = get_art_abs(story_file)\n",
        "\n",
        "            # Write to tf.Example\n",
        "            if bytes(article, 'utf-8') == 0:\n",
        "                print('error!')\n",
        "                \n",
        "            tf_example = example_pb2.Example()\n",
        "            tf_example.features.feature['article'].bytes_list.value.extend([bytes(article, 'utf-8')])\n",
        "            tf_example.features.feature['abstract'].bytes_list.value.extend([bytes(abstract, 'utf-8')])\n",
        "            tf_example_str = tf_example.SerializeToString()\n",
        "            str_len = len(tf_example_str)\n",
        "            writer.write(struct.pack('q', str_len))\n",
        "            writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
        "\n",
        "            # Write the vocab to file, if applicable\n",
        "            if makevocab:\n",
        "                art_tokens = article.split(' ')\n",
        "                abs_tokens = abstract.split(' ')\n",
        "                abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
        "                tokens = art_tokens + abs_tokens\n",
        "                tokens = [t.strip() for t in tokens] # strip\n",
        "                tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
        "                vocab_counter.update(tokens)\n",
        "\n",
        "    print(f\"Finished writing file {out_file}\\n\") \n",
        "    # write vocab to file\n",
        "    if makevocab:\n",
        "        print (\"Writing vocab file...\")\n",
        "        with open(os.path.join(finished_files_dir, \"vocab\"), 'w') as writer:\n",
        "            for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
        "                writer.write(word + ' ' + str(count) + '\\n')\n",
        "        print (\"Finished writing vocab file\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z30cqhgD-og9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "862d6873-9ddd-44bb-c7f9-e330c30f3608"
      },
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Drive/TS/TS/stanford-corenlp-full-2018-10-05'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "WF8cTAM--qDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir dataFiles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2R434X1x-qA3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e86b6683-ed3d-47ac-cea3-9e8f9ac00233"
      },
      "cell_type": "code",
      "source": [
        "cd dataFiles"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/dataFiles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKGexNPZ-p-C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir  test_tokenized test_processed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FMkHIdE-p7B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u3Q7EsA9-ZGo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stories_dir = \"/content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/TotalStories\"\n",
        "tokenized_stories_dir = \"/content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/TotalTokenized\"\n",
        "processed_dir = \"//content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/dataFiles/test_processed\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gHozpZvHAVuM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train,test,val = split_data(stories_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvhYURE4AX7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d0695b32-91bb-4051-8991-179492ea7c94"
      },
      "cell_type": "code",
      "source": [
        "test"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['35f0e33de7923036a97ac245d899f990bda5e242.story',\n",
              " '55a6aabd120f0b18297f7efe22452f9b3aca0d8d.story',\n",
              " '7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a.story',\n",
              " '0a0ae32d5581f71acb87e191a98ac27b497b232e.story',\n",
              " '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4.story',\n",
              " '7c0e61ac829a3b3b653e2e3e7536cc4881d1f264.story',\n",
              " '30cb674030b01014d2c87b92f09152857e249631.story',\n",
              " '0a0b1eeeeb4f2179293e39d36b6fac0cef271c40.story',\n",
              " '0a0bd6bed053b3bcc3414c3ef29e4aadaaf3d5b0.story']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "C7zp1k8xAZqs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "122975fd-e1ae-4d4a-8d64-5423c747199e"
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['017d27d00eb43678c15cb4a8dd4723a035323219.story',\n",
              " '77d7c8cf2a9432e395d629371a12790c563c19f7.story',\n",
              " '0a0b3d0b0ab9ee056856cdb241a58fc1ea049d53.story',\n",
              " '7fa8d9beca50155fd3d71a9087a4311ce321c34b.story',\n",
              " '9d7fc7fd9ffbfecb1c458b9929fa02e6c3538368.story',\n",
              " '0a0a007398ab02d4bd3c1ef9e034f567afa76b2c.story',\n",
              " '0a0b2b48245b8e585fed35eb89f69a8b1f0ee25a.story',\n",
              " '0a0afa9106783f8b7aca75d53054b51ca7293fb2.story',\n",
              " '2f43e9dfaa43ffbddbce339a8b6403ddce43b38a.story',\n",
              " '2ad31cae96512af5105b9b23f9b681dc732b2605.story',\n",
              " '0d43b97000ff852282c89d8d105e41495c0ee9bd.story',\n",
              " '0a0ad513d8b3f9ae5afd2d1caeb9d85c6236fa66.story',\n",
              " '06bc2f0272d033b123e9eebbb221c62d7d51c283.story',\n",
              " '0a0ad50bb4952bdfca93ff74565f1ac80e740278.story',\n",
              " '0a0a733db965c3fdf9bc2895104a1ef884a3d593.story']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "Pm5Zs3buAaX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b34cdf71-54fc-4d33-8878-bdec01295aa9"
      },
      "cell_type": "code",
      "source": [
        "val"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0a0a09d3e5a0f04d792a7d950af976ed7f1a2de7.story',\n",
              " '0a0a1a0e94aac65f2b50815050dfaccf521dda35.story',\n",
              " '0a0ba0e482cb72efffc0710e5479659fc23d346d.story',\n",
              " '0a0beabdef4146beb1171dd9f927ad70c2a08317.story',\n",
              " '5e22bbfc7232418b8d2dd646b952e404df5bd048.story',\n",
              " '0a00a9aebcb754c51534867cf1db2335dcb76884.story']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "NbT0hoBqAbIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5636775f-ebac-4175-8290-8b2c46b9c241"
      },
      "cell_type": "code",
      "source": [
        "tokenize_stories(stories_dir, tokenized_stories_dir)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing to tokenize /content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/TotalStories to /content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/TotalTokenized...\n",
            "Making list of files to tokenize...\n",
            "Tokenizing 30 files in /content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/TotalStories and saving in /content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/TotalTokenized...\n",
            "Stanford CoreNLP Tokenizer has finished.\n",
            "Successfully finished tokenizing /content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/TotalStories to /content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/TotalTokenized.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EEvhP6b3AdcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_out_file = os.path.join(processed_dir, \"train.bin\")\n",
        "test_out_file = os.path.join(processed_dir, \"test.bin\")\n",
        "validation_out_file = os.path.join(processed_dir, \"validation.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-YrC12Y5AhwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b30c3ad3-f116-457a-e963-45c1257de194"
      },
      "cell_type": "code",
      "source": [
        "write_to_bin(train,tokenized_stories_dir,train_out_file,processed_dir, makevocab=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing story 0 of 15; 0.0 percent done\n",
            "Finished writing file //content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/dataFiles/test_processed/train.bin\n",
            "\n",
            "Writing vocab file...\n",
            "Finished writing vocab file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2zx4CckdAkIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1f07fcb9-154b-4e51-994a-8047c8c53312"
      },
      "cell_type": "code",
      "source": [
        "write_to_bin(test,tokenized_stories_dir,test_out_file,processed_dir, makevocab=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing story 0 of 9; 0.0 percent done\n",
            "Finished writing file //content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/dataFiles/test_processed/test.bin\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pCIhddndBIvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "864dd6b0-f3dd-4bb0-9118-ff384238fc25"
      },
      "cell_type": "code",
      "source": [
        "write_to_bin(val,tokenized_stories_dir,validation_out_file,processed_dir, makevocab=False)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing story 0 of 6; 0.0 percent done\n",
            "Finished writing file //content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/dataFiles/test_processed/validation.bin\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pPIaITUXBK2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "364f667d-15c2-4737-ce5f-d5f5cf761f3b"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mtest_processed\u001b[0m/  \u001b[01;34mtest_tokenized\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dJGANWx6BN5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8de4ab2d-6526-40d0-db2d-fc7b59f1a908"
      },
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/dataFiles'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "HtKQP8NLB8fU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ee1e912-e9ae-47d8-97ab-64d95129eef4"
      },
      "cell_type": "code",
      "source": [
        "cd test_processed/"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Drive/TS/TS/stanford-corenlp-full-2018-10-05/dataFiles/test_processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eLJtxUlhDO22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}