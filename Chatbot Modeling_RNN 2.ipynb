{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot RNN Modeling_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainniee/TextSummarization_PointerGenerator/blob/master/Chatbot%20Modeling_RNN%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "J_sAVVVKcUT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Tokenized Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Abh7Bqllb5eU",
        "colab_type": "code",
        "outputId": "5dc91553-4ac6-4fba-9ea3-db672d9fe228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eZnarV-5cdoy",
        "colab_type": "code",
        "outputId": "abd4cf74-f7f7-49a8-c98d-079dd7dd4f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"drive/My Drive/Assignment 4 Chatbot/articles_abstract_pairs/article-abstract-tokenized.csv\")\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>shakespeares folios sell auction 25 m</td>\n",
              "      <td>copies william shakespeares books dubbed holy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>grandmothers death saved life debt</td>\n",
              "      <td>debt 20 000 source college credit cards estima...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>feared life lacked meaning . cancer pushed find</td>\n",
              "      <td>late . drunk nearing 35th birthday past dank c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>texas man serving life sentence innocent doubl...</td>\n",
              "      <td>central texas man serving life sentence double...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dads reagan protests inspire stand donald trump</td>\n",
              "      <td>battling depression sleeplessness thinking fig...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract  \\\n",
              "0             shakespeares folios sell auction 25 m    \n",
              "1                grandmothers death saved life debt    \n",
              "2   feared life lacked meaning . cancer pushed find    \n",
              "3  texas man serving life sentence innocent doubl...   \n",
              "4   dads reagan protests inspire stand donald trump    \n",
              "\n",
              "                                             article  \n",
              "0  copies william shakespeares books dubbed holy ...  \n",
              "1  debt 20 000 source college credit cards estima...  \n",
              "2  late . drunk nearing 35th birthday past dank c...  \n",
              "3  central texas man serving life sentence double...  \n",
              "4  battling depression sleeplessness thinking fig...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "n1V-8-j3ciXI",
        "colab_type": "code",
        "outputId": "0d190a95-50ed-4074-97ad-a17ed1728624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Dense, Input, RepeatVector, concatenate, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.ops.rnn import dynamic_rnn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fbt6bLwycmCG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RNN_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "KEEP_PROBABILITY = 0.5\n",
        "OPTIMIZER_TYPE = 'adam'\n",
        "LEARNING_RATE = 0.001\n",
        "EMBEDDING_SIZE = 100\n",
        "\n",
        "data_dir_path = 'drive/My Drive/Assignment 4 Chatbot'\n",
        "data_file = '/articles_abstract_pairs/article-abstract-tokenized.csv'\n",
        "glove_file = '/articles_abstract_pairs/glove.6B.' + str(EMBEDDING_SIZE) + 'd.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UOYukKKecntf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VERBOSE = 1\n",
        "NUM_LAYERS = 3\n",
        "MAX_INPUT_SEQ_LENGTH = 500\n",
        "MAX_TARGET_SEQ_LENGTH = 50\n",
        "MAX_INPUT_VOCAB_SIZE = 3000\n",
        "MAX_TARGET_VOCAB_SIZE = 1000\n",
        "NUM_SAMPLES = 5000\n",
        "MAX_DECODER_SEQ_LENGTH = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u-b6JIX4cqTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_keras_optimizer():\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, momentum=0.0, decay=0.0, nesterov=False)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        keras_optimizer = keras.optimizers.Adagrad(lr=LEARNING_RATE, epsilon=None, decay=0.0)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        keras_optimizer = keras.optimizers.Adadelta(lr=LEARNING_RATE, rho=0.95, epsilon=None, decay=0.0)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        keras_optimizer = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                                                amsgrad=False)\n",
        "\n",
        "    return keras_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Js3G9Xsct1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def def_tf_optimizer(lr):\n",
        "    if OPTIMIZER_TYPE == 'sgd':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'rmsprop':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.RMSPropOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adagrad':\n",
        "        # default LEARNING_RATE = 0.01\n",
        "        tf_optimizer = tf.train.AdagradOptimizer(lr)\n",
        "    elif OPTIMIZER_TYPE == 'adadelta':\n",
        "        # default LEARNING_RATE = 1.0\n",
        "        tf_optimizer = tf.train.AdadeltaOptimizer(lr)\n",
        "    else:   # OPTIMIZER_TYPE == 'adam':\n",
        "        # default LEARNING_RATE = 0.001\n",
        "        tf_optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    return tf_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xn7-R8qBc4BM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN Modeling"
      ]
    },
    {
      "metadata": {
        "id": "AtPCVskVczMU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_text(x, y, input_seq_max_length=None, target_seq_max_length=None):\n",
        "    if input_seq_max_length is None:\n",
        "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
        "    if target_seq_max_length is None:\n",
        "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
        "    input_counter = Counter()\n",
        "    target_counter = Counter()\n",
        "    max_input_seq_length = 0\n",
        "    max_target_seq_length = 0\n",
        "\n",
        "    for line in x:\n",
        "        text = [word for word in line.split(' ')]\n",
        "        for i, word in enumerate(text):\n",
        "            if word == '':\n",
        "                del text[i]\n",
        "        seq_length = len(text)\n",
        "        if seq_length > input_seq_max_length:\n",
        "            text = text[0:input_seq_max_length]\n",
        "            seq_length = len(text)\n",
        "        for word in text:\n",
        "            input_counter[word] += 1\n",
        "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
        "\n",
        "    for i, line in enumerate(y):\n",
        "\n",
        "        line2 = 'START ' + str(line) + ' END'\n",
        "        text = [word for word in line2.split(' ')]\n",
        "        for j, word in enumerate(text):\n",
        "            if word == '':\n",
        "                del text[j]\n",
        "        seq_length = len(text)\n",
        "        if seq_length > target_seq_max_length:\n",
        "            text = text[0:target_seq_max_length]\n",
        "            seq_length = len(text)\n",
        "        for word in text:\n",
        "            target_counter[word] += 1\n",
        "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
        "\n",
        "    input_word2idx = dict()\n",
        "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
        "        input_word2idx[word[0]] = idx + 2\n",
        "    input_word2idx['PAD'] = 0\n",
        "    input_word2idx['UNK'] = 1\n",
        "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
        "\n",
        "    target_word2idx = dict()\n",
        "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
        "        target_word2idx[word[0]] = idx + 1\n",
        "    target_word2idx['UNK'] = 0\n",
        "\n",
        "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "\n",
        "    num_input_tokens = len(input_word2idx)\n",
        "    num_target_tokens = len(target_word2idx)\n",
        "\n",
        "    config = dict()\n",
        "    config['input_word2idx'] = input_word2idx\n",
        "    config['input_idx2word'] = input_idx2word\n",
        "    config['target_word2idx'] = target_word2idx\n",
        "    config['target_idx2word'] = target_idx2word\n",
        "    config['num_input_tokens'] = num_input_tokens\n",
        "    config['num_target_tokens'] = num_target_tokens\n",
        "    config['max_input_seq_length'] = max_input_seq_length\n",
        "    config['max_target_seq_length'] = max_target_seq_length\n",
        "\n",
        "    return config\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYQuFcfAc6Z5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_input_text(texts, input_word2idx, max_input_seq_length):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = []\n",
        "        for word in line.lower().split(' '):\n",
        "            wid = 1\n",
        "            if word in input_word2idx:\n",
        "                wid = input_word2idx[word]\n",
        "            x.append(wid)\n",
        "            if len(x) >= max_input_seq_length:\n",
        "                break\n",
        "        temp.append(x)\n",
        "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
        "\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FADi9pm_c9AK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_target_encoding(texts, max_target_seq_length):\n",
        "    temp = []\n",
        "    for line in texts:\n",
        "        x = []\n",
        "        line2 = 'START ' + line.lower() + ' END'\n",
        "        for word in line2.split(' '):\n",
        "            x.append(word)\n",
        "            if len(x) >= max_target_seq_length:\n",
        "                break\n",
        "        temp.append(x)\n",
        "\n",
        "    temp = np.array(temp)\n",
        "    print(temp.shape)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-usOUnsYc_Qr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RecursiveRNN(object):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.num_input_tokens = config['num_input_tokens']\n",
        "        self.max_input_seq_length = config['max_input_seq_length']\n",
        "        self.num_target_tokens = config['num_target_tokens']\n",
        "        self.max_target_seq_length = config['max_target_seq_length']\n",
        "        self.input_word2idx = config['input_word2idx']\n",
        "        self.input_idx2word = config['input_idx2word']\n",
        "        self.target_word2idx = config['target_word2idx']\n",
        "        self.target_idx2word = config['target_idx2word']\n",
        "        self.config = config \n",
        "        \n",
        "        # article input model\n",
        "        inputs1 = Input(shape=(self.max_input_seq_length,))\n",
        "        article1 = Embedding(self.num_input_tokens, 128)(inputs1)\n",
        "        article2 = Dropout(0.3)(article1)\n",
        "\n",
        "        # summary input model\n",
        "        inputs2 = Input(shape=(min(self.num_target_tokens, MAX_DECODER_SEQ_LENGTH),))\n",
        "        summ1 = Embedding(self.num_target_tokens, 128)(inputs2)\n",
        "        summ2 = Dropout(0.3)(summ1)\n",
        "        summ3 = LSTM(128)(summ2)\n",
        "        summ4 = RepeatVector(self.max_input_seq_length)(summ3)\n",
        "\n",
        "        # decoder model\n",
        "        decoder1 = concatenate([article2, summ4])\n",
        "        decoder2 = LSTM(128)(decoder1)\n",
        "        outputs = Dense(self.num_target_tokens, activation='softmax')(decoder2)     \n",
        "\n",
        "        # # tie it together [article, summary] [word]\n",
        "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "        optimizer = def_keras_optimizer()\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.model = model\n",
        "\n",
        "    def generate_batch(self, x_samples, y_samples, batch_size):\n",
        "        encoder_input_data_batch = []\n",
        "        decoder_input_data_batch = []\n",
        "        decoder_target_data_batch = []\n",
        "        line_idx = 0\n",
        "        while True:\n",
        "            for recordIdx in range(0, len(x_samples)):\n",
        "                target_words = y_samples[recordIdx]\n",
        "                x = x_samples[recordIdx]\n",
        "                decoder_input_line = []\n",
        "\n",
        "                for idx in range(0, len(target_words) - 1):\n",
        "                    w2idx = 0  # default [UNK]\n",
        "                    w = target_words[idx]\n",
        "                    if w in self.target_word2idx:\n",
        "                        w2idx = self.target_word2idx[w]\n",
        "                    decoder_input_line = decoder_input_line + [w2idx]\n",
        "                    decoder_target_label = np.zeros(self.num_target_tokens)\n",
        "                    w2idx_next = 0\n",
        "                    if target_words[idx + 1] in self.target_word2idx:\n",
        "                        w2idx_next = self.target_word2idx[target_words[idx + 1]]\n",
        "                    if w2idx_next != 0:\n",
        "                        decoder_target_label[w2idx_next] = 1\n",
        "                    decoder_input_data_batch.append(decoder_input_line)\n",
        "                    encoder_input_data_batch.append(x)\n",
        "                    decoder_target_data_batch.append(decoder_target_label)\n",
        "\n",
        "                    line_idx += 1\n",
        "                    if line_idx >= batch_size:\n",
        "                      yield [pad_sequences(encoder_input_data_batch, self.max_input_seq_length), pad_sequences(\n",
        "                          decoder_input_data_batch, min(self.num_target_tokens, MAX_DECODER_SEQ_LENGTH))], \\\n",
        "                      np.array(decoder_target_data_batch)\n",
        "                      line_idx = 0\n",
        "                      encoder_input_data_batch = []\n",
        "                      decoder_input_data_batch = []\n",
        "                      decoder_target_data_batch = []\n",
        "\n",
        "    def fit(self, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
        "\n",
        "        y_train = transform_target_encoding(y_train, self.max_target_seq_length)\n",
        "        y_test = transform_target_encoding(y_test, self.max_target_seq_length)\n",
        "\n",
        "        x_train = transform_input_text(x_train, self.input_word2idx, self.max_input_seq_length)\n",
        "        x_test = transform_input_text(x_test, self.input_word2idx, self.max_input_seq_length)\n",
        "\n",
        "        train_gen = self.generate_batch(x_train, y_train, batch_size)\n",
        "        test_gen = self.generate_batch(x_test, y_test, batch_size)\n",
        "\n",
        "        total_training_samples = sum([len(target_text) - 1 for target_text in y_train])\n",
        "        total_testing_samples = sum([len(target_text) - 1 for target_text in y_test])\n",
        "        train_num_batches = total_training_samples // batch_size\n",
        "        test_num_batches = total_testing_samples // batch_size\n",
        "\n",
        "        self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches, epochs=epochs, verbose=VERBOSE,\n",
        "                                 validation_data=test_gen, validation_steps=test_num_batches)\n",
        "\n",
        "    def summarize(self, input_text):\n",
        "        input_seq = []\n",
        "        input_wids = []\n",
        "        for word in input_text.lower().split(' '):\n",
        "            idx = 1  # default [UNK]\n",
        "            if word in self.input_word2idx:\n",
        "                idx = self.input_word2idx[word]\n",
        "            input_wids.append(idx)\n",
        "        input_seq.append(input_wids)\n",
        "        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
        "        start_token = self.target_word2idx['START']\n",
        "        wid_list = [start_token]\n",
        "        \n",
        "        sum_input_seq = pad_sequences([wid_list], min(self.num_target_tokens, MAX_DECODER_SEQ_LENGTH))\n",
        "        terminated = False\n",
        "\n",
        "        target_text = ''\n",
        "\n",
        "        while not terminated:\n",
        "            output_tokens = self.model.predict([input_seq, sum_input_seq])\n",
        "            sample_token_idx = np.argmax(output_tokens[0, :])\n",
        "            sample_word = self.target_idx2word[sample_token_idx]\n",
        "            wid_list = wid_list + [sample_token_idx]\n",
        "\n",
        "            if sample_word != 'START' and sample_word != 'END':\n",
        "                target_text += ' ' + sample_word\n",
        "\n",
        "            if sample_word == 'END' or len(wid_list) >= self.max_target_seq_length:\n",
        "                terminated = True\n",
        "            else:\n",
        "                sum_input_seq = pad_sequences([wid_list],  min(self.num_target_tokens, MAX_DECODER_SEQ_LENGTH))\n",
        "                    \n",
        "        return target_text.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLWm81w3dF-0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main_rnn():\n",
        "    print('loading csv file ...')\n",
        "    df = pd.read_csv(data_dir_path + data_file)\n",
        "\n",
        "    print('extract configuration from input texts ...')\n",
        "    y = df['abstract']\n",
        "    x = df['article']\n",
        "    config = fit_text(x, y)\n",
        "\n",
        "    print('configuration extracted from input texts ...')\n",
        "\n",
        "    summarizer = RecursiveRNN(config)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print('training size: ', len(x_train))\n",
        "    print('testing size: ', len(x_test))\n",
        "\n",
        "    print('start fitting ...')\n",
        "    summarizer.fit(x_train, y_train, x_test, y_test, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    print('start predicting ...')\n",
        "    for i in np.random.permutation(np.arange(len(x)))[0:10]:\n",
        "        x = x[i]\n",
        "        actual_abstract = y[i]\n",
        "        abstract = summarizer.summarize(x)\n",
        "        print('Article: ', x)\n",
        "        print('Generated Abstract: ', abstract)\n",
        "        print('Original Abstract: ', actual_abstract)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PuKdat1Ad7Hu",
        "colab_type": "code",
        "outputId": "c7bb59ae-10eb-410e-e15a-924da8381084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "cell_type": "code",
      "source": [
        "main_rnn()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading csv file ...\n",
            "extract configuration from input texts ...\n",
            "configuration extracted from input texts ...\n",
            "training size:  34038\n",
            "testing size:  8510\n",
            "start fitting ...\n",
            "(34038,)\n",
            "(8510,)\n",
            "(34038, 213)\n",
            "(8510, 213)\n",
            "Epoch 1/3\n",
            "4960/4960 [==============================] - 3234s 652ms/step - loss: 2.6708 - acc: 0.1228 - val_loss: 2.5992 - val_acc: 0.1275\n",
            "Epoch 2/3\n",
            "4960/4960 [==============================] - 2999s 605ms/step - loss: 2.5522 - acc: 0.1315 - val_loss: 2.5242 - val_acc: 0.1351\n",
            "Epoch 3/3\n",
            "4960/4960 [==============================] - 3485s 703ms/step - loss: 2.4743 - acc: 0.1375 - val_loss: 2.4763 - val_acc: 0.1401\n",
            "start predicting ...\n",
            "Article:  rotation cyclical sectors sp 500 away defensive stocks continue week federal reserve chair janet yellen strikes tone welcoming interest rate increase later year . fed speakers week including influential new york fed president william dudley blaze trail yellen upcoming friday speech jackson hole wyoming global meeting central bankers \n",
            "Generated Abstract:  trump administration says trump pick new york chief says trump says trump pick new york chief says trump says trump pick\n",
            "Original Abstract:  new york nabs global property crown london brexit fears \n",
            "Article:  r\n",
            "Generated Abstract:  trump says trump administration says trump says trump says trump says trump says trump says trump says trump says trump says\n",
            "Original Abstract:  americas problems nt obamas fault . george w bushs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-da1f352b032b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-5588b78e2282>\u001b[0m in \u001b[0;36mmain_rnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start predicting ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mactual_abstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: string index out of range"
          ]
        }
      ]
    }
  ]
}